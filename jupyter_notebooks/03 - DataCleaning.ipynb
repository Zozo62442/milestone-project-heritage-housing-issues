{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b94e4d2",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9e2e0",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Prepare the dataset for ML by:\n",
    "  * Assessing and handling missing data.\n",
    "  * Removing irrelevant or problematic features.\n",
    "  * Ensuring consistency in data types.\n",
    "  * Saving clean datasets for training and testing.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* `outputs/datasets/cleaned/df_main_for_cleaning.csv`: output from the EDA notebook, ready for cleaning.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* `outputs/datasets/cleaned/TrainSetCleaned.csv`: cleaned training dataset.\n",
    "* `outputs/datasets/cleaned/TestSetCleaned.csv`: cleaned test dataset.\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* This notebook focuses solely on the cleaned house records (`df_main`).  \n",
    "* `inherited_houses.csv` is kept separate and will be used later for prediction once the model is trained.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383ade3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348d660",
   "metadata": {},
   "source": [
    "# Change Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f835da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7655fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d32ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35018d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417c058",
   "metadata": {},
   "source": [
    "# Load Collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset prepared during EDA\n",
    "file_path = \"outputs/datasets/cleaned/df_main_for_cleaning.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473eb45",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d3f53",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute / len(df) * 100, 2)\n",
    "\n",
    "    df_missing_data = (\n",
    "        pd.DataFrame({\n",
    "            \"RowsWithMissingData\": missing_data_absolute,\n",
    "            \"PercentageOfDataset\": missing_data_percentage,\n",
    "            \"DataType\": df.dtypes\n",
    "        })\n",
    "        .sort_values(by='PercentageOfDataset', ascending=False)\n",
    "        .query(\"PercentageOfDataset > 0\")\n",
    "    )\n",
    "\n",
    "    return df_missing_data\n",
    "\n",
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa232b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"✅ There are no variables with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960b4a0",
   "metadata": {},
   "source": [
    "# Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b796e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, mask=mask, cmap='viridis',\n",
    "                    annot_kws={\"size\": font_annot}, ax=ax, linewidth=0.5)\n",
    "        ax.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, mask=mask, cmap='rocket_r',\n",
    "                    annot_kws={\"size\": font_annot}, ax=ax, linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
    "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold):\n",
    "    print(\"*** Spearman Correlation ***\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=(12, 10), font_annot=10)\n",
    "\n",
    "    print(\"*** Pearson Correlation ***\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=(12, 10), font_annot=10)\n",
    "\n",
    "    print(\"*** Power Predictive Score (PPS) ***\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=(12, 10), font_annot=10)\n",
    "\n",
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)\n",
    "DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold=0.4, PPS_Threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28fbd4",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a561eb",
   "metadata": {},
   "source": [
    "Strategy to Drop Variables (if any)\n",
    "We drop columns if:\n",
    "* More than 80% missing values\n",
    "* Duplicated or irrelevant (e.g., unique IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55551b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "features_to_drop = [] # depending on previous output\n",
    "\n",
    "print(f\"* {len(features_to_drop)} variables to drop \\n\\n{features_to_drop}\")\n",
    "\n",
    "dropper = DropFeatures(features_to_drop=features_to_drop)\n",
    "df = dropper.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30c057",
   "metadata": {},
   "source": [
    "Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Fill categorical with mode\n",
    "df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Fill numerical with median\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000414c",
   "metadata": {},
   "source": [
    "# SPlit Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a151db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TrainSet, TestSet = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"✅ TrainSet shape: {TrainSet.shape}\")\n",
    "print(f\"✅ TestSet shape: {TestSet.shape}\")\n",
    "\n",
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6b201",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c7eb9",
   "metadata": {},
   "source": [
    "# Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.makedirs(\"outputs/datasets/cleaned\", exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee65e36f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87696e77",
   "metadata": {},
   "source": [
    "# Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7d229",
   "metadata": {},
   "source": [
    "* Imputed all missing numerical values using median\n",
    "* Imputed categorical columns with most frequent value\n",
    "* Saved TrainSetCleaned and TestSetCleaned"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
